<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Marginal <span class="caps">GP</span> model in&nbsp;PyMC4</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="Tirth Patel">

    <!-- Le styles -->
    <link rel="stylesheet" href="/theme/css/bootstrap.min.css" type="text/css" />
    <style type="text/css">
      body {
        padding-top: 60px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
      .tag-1 {
        font-size: 13pt;
      }
      .tag-2 {
        font-size: 10pt;
      }
      .tag-2 {
        font-size: 8pt;
      }
      .tag-4 {
        font-size: 6pt;
     }
    </style>
    <link href="/theme/css/bootstrap-responsive.min.css" rel="stylesheet">
        <link href="/theme/css/font-awesome.css" rel="stylesheet">

    <link href="/theme/css/pygments.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="/theme/images/favicon.ico">
    <link rel="apple-touch-icon" href="/theme/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/theme/images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/theme/images/apple-touch-icon-114x114.png">

    <link href="/" type="application/atom+xml" rel="alternate" title="Tirth Patel ATOM Feed" />

  </head>

  <body>

    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="/index.html">Tirth Patel </a>
          <div class="nav-collapse">
            <ul class="nav">
                <li><a href="/about/">About</a></li>
                <li><a href="/gsoc-2020/">GSoC&nbsp;2020</a></li>
                <li><a href="/gsoc-2021/">GSoC&nbsp;2021</a></li>
                          <li class="divider-vertical"></li>

                          <ul class="nav pull-right">
                                <li><a href="/archives.html"><i class="icon-th-list"></i>Archives</a></li>
                          </ul>

            </ul>
            <!--<p class="navbar-text pull-right">Logged in as <a href="#">username</a></p>-->
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row">
        <div class="span9" id="content">
<section id="content">
        <article>
                <div class="alert">
                        <span class="closebtn" onclick="this.parentElement.style.display='none';">&times;</span>
                        <strong>Note</strong>: This article has been migrated from my previous website without modification; some
                        images might be missing or be disproportionately large.
                </div>
                <header>
                        <h1>
                                <a href=""
                                        rel="bookmark"
                                        title="Permalink to Marginal GP model inÂ PyMC4">
                                        Marginal <span class="caps">GP</span> model in&nbsp;PyMC4
                                </a>
                        </h1>
                </header>
                <div class="entry-content">
                <div class="well">
<footer class="post-info">
<span class="label">Date</span>
<abbr class="published" title="2020-08-10T00:00:00+05:30">
        <i class="icon-calendar"></i>Mon 10 August 2020
</abbr>
<span class="label">By</span>
<a href="/author/tirth-patel/"><i class="icon-user"></i>Tirth Patel</a>
<span class="label">Category</span>
<a href="/category/gsoc-2020/"><i class="icon-folder-open"></i>GSoC 2020</a>.


<span class="label">Tags</span>
	<a href="/tag/gsoc-2020/"><i class="icon-tag"></i>gsoc 2020</a>
	<a href="/tag/machine-learning/"><i class="icon-tag"></i>machine learning</a>
	<a href="/tag/gaussian-processes/"><i class="icon-tag"></i>gaussian processes</a>
</footer><!-- /.post-info -->                </div>
                <h2>Marginal Gaussian&nbsp;Process</h2>
<h3>The&nbsp;Model</h3>
<p>A Marginal Gaussian process jointly represents the data as a large probability distribution. This distribution, as we will see, turns out to a normal distribution in classical Bayesian approaches. Suppose we have some data <span class="math">\(X, y\)</span> using which we want to predict the distribution over some <span class="math">\(y_*\)</span> given new data <span class="math">\(X_*\)</span>. We write this&nbsp;as:</p>
<div class="math">$$P(y_* \mid X, y, X_*)$$</div>
<p>This distribution is known as the <strong><em>conditional</em></strong>&nbsp;distribution.</p>
<p>We can construct this joint density through the use of the following&nbsp;decomposition:</p>
<div class="math">$$P(y_* \mid X, y, X_*) = \int{ P(y_* \mid X_*, \theta) P(\theta \mid X, y) }$$</div>
<p>where we represent all the parameters by <span class="math">\(\theta\)</span> and <span class="math">\(P(\theta \mid X, y)\)</span> is the posterior density given data and <span class="math">\(P(y_* \mid X_*, \theta)\)</span> is known as the <strong><em>marginal likelihood</em></strong> of the individual test point given the&nbsp;parameters.</p>
<p>The marginal likelihood of the model is assumed to be a gaussian with&nbsp;parameters:</p>
<div class="math">$$P(y \mid X, \theta) = \mathcal{N}(m(X), K(X, X) + \delta(\epsilon))$$</div>
<p>where <span class="math">\(m(\cdot)\)</span> is the <strong><em>mean</em></strong> function and <span class="math">\(K(\cdot)\)</span> is the <strong><em>kernel</em></strong> function. The mean function evaluates a mean vector. The kernel function takes some parameters <span class="math">\(\theta\)</span>, evaluates the covariance between every pair of data points, and outputs a covariance matrix. The <span class="math">\(\delta(\cdot)\)</span> is the Kronecker delta function and <span class="math">\(\epsilon\)</span> is a small noise. The delta function adds a small noise corruption and is equivalent to adding noise distributed as <span class="math">\(\mathcal{N}(0, \epsilon\mathrm{I})\)</span>.</p>
<p>This mean vector and the covariance matrix is then input to the multivariate normal distribution. This choice of using a normal distribution makes the integral in the conditional distribution analytical, hence, making it easy to infer a distribution over the test data for prediction and&nbsp;generation.</p>
<h3>Performing&nbsp;Inference</h3>
<p>The conditional distribution can also be shown to be a normal distribution with&nbsp;parameters:</p>
<div class="math">$$P(y_* \mid X, y, X_*) = \mathcal{N}(\mu_{y_* \mid D}, \Sigma_{y_* \mid D})$$</div>
<p>where</p>
<div class="math">$$
\begin{align*}
\mu_{y_* \mid D}    &amp;= K_*^T \left( K + \epsilon\mathrm{I} \right)^{-1} y \\
\Sigma_{y_* \mid D} &amp;= K_{**} - K_*^T \left( K + \epsilon\mathrm{I} \right)^{-1} K_* \\
K_*                 &amp;= K(X, X_*) \\
K_{**}              &amp;= K(X_*, X_*) \\
K                   &amp;= K(X, X)
\end{align*}
$$</div>
<p>This distribution can be used to make predictions over the test data or generate predictive&nbsp;samples.</p>
<h3>References</h3>
<ul>
<li>http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote15.html</li>
<li>http://inverseprobability.com/talks/notes/gaussian-processes.html</li>
</ul>
<h1>MarginalGP&nbsp;Model</h1>
<p><code>MarginalGP</code> model is the implementation of the Marginal <span class="caps">GP</span> model in PyMC4. It contains <code>marginal_likelihood</code> and <code>conditional</code> method that does exactly as described in the previous section. Moreover, it also has methods <code>predict</code> and <code>predictt</code> to sample from the conditional distribution and to get point estimate (<span class="math">\(\mu_{y_* \mid D}\)</span>) of the conditional distribution&nbsp;respectively.</p>
<p>Let&#8217;s see each method in detail in the following&nbsp;sections:</p>
<h3><code>marginal_likelihood</code> method</h3>
<p>First, we need to instantiate the model using a kernel function and (optionally) a mean&nbsp;function.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pymc4.gp</span> <span class="kn">import</span> <span class="n">MarginalGP</span>
<span class="kn">from</span> <span class="nn">pymc4.gp.cov</span> <span class="kn">import</span> <span class="n">ExpQuad</span>

<span class="c1"># Let&#39;s first instantiate a kernel</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">ExpQuad</span><span class="p">(</span><span class="n">length_scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span>

<span class="c1"># Now, we can instantiata the model</span>
<span class="n">gp</span> <span class="o">=</span> <span class="n">MarginalGP</span><span class="p">(</span><span class="n">cov_fn</span> <span class="o">=</span> <span class="n">K</span><span class="p">)</span>
</code></pre></div>

<p>Now, To get the <code>marginal_likelihood</code> of the MarginalGP over some data <code>X</code> (of shape <code>(n_samples, n_features)</code>) with labels <code>y</code> (of shape <code>(n_samples, )</code>),&nbsp;use:</p>
<div class="highlight"><pre><span></span><code><span class="n">noise</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s2">&quot;y_&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
</code></pre></div>

<p>You can also pass a covariance object as noise to the <code>marginal_likelihood</code>
method:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># kronecker delta function with epsilon 1e-2</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">WhiteNoise</span><span class="p">(</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s2">&quot;y_&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
</code></pre></div>

<p>If <code>y</code> is not the observed data, pass <code>is_observed=False</code> in the
        marginal likelihood&nbsp;method:</p>
<div class="highlight"><pre><span></span><code><span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s2">&quot;y_&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span> <span class="n">is_observed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<p>By default, some <code>jitter</code> is added to ensure Cholesky Decomposition passes.
        This behavior can be turned off by passing <code>jitter=0</code> in the marginal
        likelihood&nbsp;method:</p>
<div class="highlight"><pre><span></span><code><span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s2">&quot;y_&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<p>As <code>noise</code> behaves exactly as <code>jitter</code>, it is recommended to set <code>jitter=False</code>
        to avoid adding extra&nbsp;noise.</p>
<h3><code>conditional</code> method</h3>
<p>You can use conditional method to get the conditional distribution
        over the new data points. This distribution can be used to predict
        over the new data&nbsp;points:</p>
<div class="highlight"><pre><span></span><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">conditional</span><span class="p">(</span><span class="s2">&quot;y_pred&quot;</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="n">given</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;X&quot;</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;noise&quot;</span><span class="p">:</span> <span class="n">noise</span><span class="p">})</span>
</code></pre></div>

<p>where <code>Xnew</code> are the test points (new data points) and <code>y_pred</code> is a multivariate normal distribution. <code>given</code> dictionary is optional when <code>marginal_likelihood</code> method has been called&nbsp;before.</p>
<div class="highlight"><pre><span></span><code><span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s2">&quot;y_&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">conditional</span><span class="p">(</span><span class="s2">&quot;y_pred&quot;</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">)</span> <span class="c1"># no need to pass given</span>
</code></pre></div>

<p>To add noise in the conditional distribution,&nbsp;use:</p>
<div class="highlight"><pre><span></span><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">conditional</span><span class="p">(</span><span class="s2">&quot;y_pred&quot;</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="n">pred_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

<p>To avoid reparametrizing to the <code>MvNormalCholesky</code> distribution,&nbsp;use:</p>
<div class="highlight"><pre><span></span><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">conditional</span><span class="p">(</span><span class="s2">&quot;y_pred&quot;</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="n">reparametrize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<h3>Notes</h3>
<p>The data and the parameters <strong>must have the same datatype</strong>. For example, if the data is represented as <code>float64</code> then all the parameters must also be represented as <code>float64</code> datatype only. Using different datatypes in-between the model will raise an&nbsp;exception.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc4</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">from</span> <span class="nn">pymc4.gp</span> <span class="kn">import</span> <span class="n">MarginalGP</span>
<span class="kn">from</span> <span class="nn">pymc4.gp.cov</span> <span class="kn">import</span> <span class="n">ExpQuad</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">KernelPCA</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">&#39;retina&#39;</span>
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">8927</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;arviz-darkgrid&#39;</span><span class="p">)</span>
</code></pre></div>

<h3>Example 1 : <span class="caps">GP</span>-<span class="caps">LVM</span> using a Linear&nbsp;Kernel</h3>
<p>Marginal Gaussian Processes can be used to perform Principal Component Analysis (<span class="caps">PCA</span>) (a technique to project and visualize high dimensional data onto a low dimensional feature space) using a model called Gaussian Process Latent Variable Model (<span class="caps">GP</span>-<span class="caps">LVM</span>). <span class="caps">GP</span>-LVMs generally outperform the native <span class="caps">PCS</span> algorithm and can even generalize further using non-linear kernel functions like <span class="caps">RBF</span> kernel. This property of <span class="caps">GP</span>-LVMs makes them more auspicious over vanilla <span class="caps">PCA</span>. To learn more about <span class="caps">GP</span>-LVMs, refer&nbsp;[1].</p>
<p>Below is an example of a Linear <span class="caps">GP</span>-<span class="caps">LVM</span> that projects the Iris Flower Dataset onto two-dimensional feature space while preserving a clear separation between different types of&nbsp;flowers.</p>
<p><strong>References</strong></p>
<p>[1] Neil D. Lawrence, 2003, Gaussian process latent variable models for visualization of high dimensional data,&nbsp;https://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Load the data. Needs sklearn - Use `pip install sklearn` inside the python</span>
<span class="c1"># environment/conda prompt/terminal to install sklearn locally. </span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="n">N</span><span class="p">,</span> <span class="n">P</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># Linear GP-LVM model</span>
<span class="nd">@pm</span><span class="o">.</span><span class="n">model</span>
<span class="k">def</span> <span class="nf">GPLVM</span><span class="p">():</span>
    <span class="c1"># A normal prior over the projected data</span>
    <span class="n">x</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="o">.</span><span class="n">MvNormalCholesky</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">batch_stack</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Prior over the argumemnts of the covariance function.</span>
    <span class="n">args</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;args&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">5.</span><span class="p">),</span> <span class="n">batch_stack</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">cov_fn</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">cov</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">MarginalGP</span><span class="p">(</span><span class="n">cov_fn</span><span class="o">=</span><span class="n">cov_fn</span><span class="p">)</span>

    <span class="c1"># We put a marginal likelihood over every feature in the dataset.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">gp</span><span class="o">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">noise</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.01</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPLVM</span><span class="p">()</span>
<span class="n">advi</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="mi">15000</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">advi</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of iterations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Negative ELBO&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">trace</span> <span class="o">=</span> <span class="n">advi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">15000</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;GPLVM/x&#39;</span><span class="p">])</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">xx</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xx</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="png" src="/images/GP-Marginal_files/GP-Marginal_6_1.png"></p>
<p><img alt="png" src="/images/GP-Marginal_files/GP-Marginal_6_2.png"></p>
<h3><span class="caps">GP</span>-LVMs with a non-linear&nbsp;kernel</h3>
<p>Below is the demonstration of a <span class="caps">GP</span>-<span class="caps">LVM</span> using a non-linear <span class="caps">RBF</span> kernel and a comparison with the sklearn&#8217;s <code>KernelPCA</code> model.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Experiments with Non-Linear Kernels</span>
<span class="nd">@pm</span><span class="o">.</span><span class="n">model</span>
<span class="k">def</span> <span class="nf">GPLVM</span><span class="p">():</span>
    <span class="c1"># A normal prior over the projected data</span>
    <span class="n">x</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="o">.</span><span class="n">MvNormalCholesky</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">batch_stack</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Prior over the argumemnts of the covariance function.</span>
    <span class="n">args</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;args&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">100.0</span><span class="p">),</span> <span class="n">batch_stack</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">cov_fn</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">cov</span><span class="o">.</span><span class="n">ExpQuad</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">MarginalGP</span><span class="p">(</span><span class="n">cov_fn</span><span class="o">=</span><span class="n">cov_fn</span><span class="p">)</span>

    <span class="c1"># We put a marginal likelihood over every feature in the dataset.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">P</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">gp</span><span class="o">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">noise</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPLVM</span><span class="p">()</span>
<span class="n">advi</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">15000</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">advi</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of iterations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Negative ELBO&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">trace</span> <span class="o">=</span> <span class="n">advi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">15000</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;GPLVM/x&#39;</span><span class="p">])</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">xx</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xx</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="png" src="/images/GP-Marginal_files/GP-Marginal_8_0.png"></p>
<p><img alt="png" src="/images/GP-Marginal_files/GP-Marginal_8_1.png"></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Comparing with sklearn</span>
<span class="n">transformer</span> <span class="o">=</span> <span class="n">KernelPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xx</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="png" src="/images/GP-Marginal_files/GP-Marginal_9_0.png"></p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
<span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>pymc4      4.0a2
arviz      0.9.0
tensorflow 2.4.0-dev20200705
numpy      1.19.0
last updated: Fri Aug 07 2020 

CPython 3.8.0
IPython 7.16.1
watermark 2.0.2
</code></pre></div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                </div><!-- /.entry-content -->
        </article>
</section>
        </div><!--/span-->

                <div class="span3 well sidebar-nav" id="sidebar">
<ul class="nav nav-list">

<li class="nav-header"><h4><i class="icon-folder-close icon-large"></i>Categories</h4></li>
<li>
<a href="/category/gsoc-2020/">
    <i class="icon-folder-open icon-large"></i>GSoC 2020
</a>
</li>
<li>
<a href="/category/gsoc-2021/">
    <i class="icon-folder-open icon-large"></i>GSoC 2021
</a>
</li>
<li>
<a href="/category/rng/">
    <i class="icon-folder-open icon-large"></i>RNG
</a>
</li>
<li>
<a href="/category/scipy/">
    <i class="icon-folder-open icon-large"></i>SciPy
</a>
</li>

<li class="nav-header"><h4><i class="icon-tags icon-large"></i>Tags</h4></li>


</ul>        </div><!--/.well -->

      </div><!--/row-->

      <hr>

      <footer>
        <address id="about">
                Proudly powered by <a href="http://pelican.notmyidea.org/">Pelican <i class="icon-external-link"></i></a>,
                                which takes great advantage of <a href="http://python.org">Python <i class="icon-external-link"></i></a>.
        </address><!-- /#about -->

        <p>The theme is from <a href="http://twitter.github.com/bootstrap/">Bootstrap from Twitter <i class="icon-external-link"></i></a>,
                   and <a href="http://fortawesome.github.com/Font-Awesome/">Font-Awesome <i class="icon-external-link"></i></a>, thanks!</p>
      </footer>

    </div><!--/.fluid-container-->



    <!-- Le javascript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="/theme/js/jquery-1.7.2.min.js"></script>
    <script src="/theme/js/bootstrap.min.js"></script>
  </body>
</html>