

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Marginal GP model in PyMC4 - Tirth Patel</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Tirth Patel">
<meta property="og:title" content="Marginal GP model in PyMC4">


  <link rel="canonical" href="http://localhost:4000/gsoc2020/marginal-gp-model-in-pymc4">
  <meta property="og:url" content="http://localhost:4000/gsoc2020/marginal-gp-model-in-pymc4">



  <meta property="og:description" content="Ungergrad student at Nirma University. GSoC’20 @NumFOCUS with PyMC3">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2020-08-10T00:00:00-07:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Tirth Patel",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Tirth Patel Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="http://localhost:4000/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="http://localhost:4000/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="http://localhost:4000/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="http://localhost:4000/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="http://localhost:4000/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="http://localhost:4000/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="http://localhost:4000/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Tirth Patel</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/year-archive/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/gsoc2020/">GSoC 2020</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/cv/">CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  



  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Marginal GP model in PyMC4">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="August 10, 2020">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Marginal GP model in PyMC4
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  7 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-08-10T00:00:00-07:00">August 10, 2020</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <h2 id="marginal-gaussian-process">Marginal Gaussian Process</h2>

<h3 id="the-model">The Model</h3>

<p>A Marginal Gaussian process jointly represents the data as a large probability distribution. This distribution, as we will see, turns out to a normal distribution in classical Bayesian approaches. Suppose we have some data $X, y$ using which we want to predict the distribution over some $y_<em>$ given new data $X_</em>$. We write this as:</p>

\[P(y_* \mid X, y, X_*)\]

<p>This distribution is known as the <strong><em>conditional</em></strong> distribution.</p>

<p>We can construct this joint density through the use of the following decomposition:</p>

\[P(y_* \mid X, y, X_*) = \int{ P(y_* \mid X_*, \theta) P(\theta \mid X, y) }\]

<p>where we represent all the parameters by $\theta$ and $P(\theta \mid X, y)$ is the posterior density given data and $P(y_* \mid X_<em>, \theta)$ is known as the <strong>*marginal likelihood</strong></em> of the individual test point given the parameters.</p>

<p>The marginal likelihood of the model is assumed to be a gaussian with parameters:</p>

\[P(y \mid X, \theta) = \mathcal{N}(m(X), K(X, X) + \delta(\epsilon))\]

<p>where $m(\cdot)$ is the <strong><em>mean</em></strong> function and $K(\cdot)$ is the <strong><em>kernel</em></strong> function. The mean function evaluates a mean vector. The kernel function takes some parameters $\theta$, evaluates the covariance between every pair of data points, and outputs a covariance matrix. The $\delta(\cdot)$ is the Kronecker delta function and $\epsilon$ is a small noise. The delta function adds a small noise corruption and is equivalent to adding noise distributed as $\mathcal{N}(0, \epsilon\mathrm{I})$.</p>

<p>This mean vector and the covariance matrix is then input to the multivariate normal distribution. This choice of using a normal distribution makes the integral in the conditional distribution analytical, hence, making it easy to infer a distribution over the test data for prediction and generation.</p>

<h3 id="performing-inference">Performing Inference</h3>

<p>The conditional distribution can also be shown to be a normal distribution with parameters:</p>

\[P(y_* \mid X, y, X_*) = \mathcal{N}(\mu_{y_* \mid D}, \Sigma_{y_* \mid D})\]

<p>where</p>

\[\begin{align*}
\mu_{y_* \mid D}    &amp;= K_*^T \left( K + \epsilon\mathrm{I} \right)^{-1} y \\
\Sigma_{y_* \mid D} &amp;= K_{**} - K_*^T \left( K + \epsilon\mathrm{I} \right)^{-1} K_* \\
K_*                 &amp;= K(X, X_*) \\
K_{**}              &amp;= K(X_*, X_*) \\
K                   &amp;= K(X, X)
\end{align*}\]

<p>This distribution can be used to make predictions over the test data or generate predictive samples.</p>

<h3 id="references">References</h3>

<ul>
  <li>http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote15.html</li>
  <li>http://inverseprobability.com/talks/notes/gaussian-processes.html</li>
</ul>

<h1 id="marginalgp-model">MarginalGP Model</h1>

<p><code class="language-plaintext highlighter-rouge">MarginalGP</code> model is the implementation of the Marginal GP model in PyMC4. It contains <code class="language-plaintext highlighter-rouge">marginal_likelihood</code> and <code class="language-plaintext highlighter-rouge">conditional</code> method that does exactly as described in the previous section. Moreover, it also has methods <code class="language-plaintext highlighter-rouge">predict</code> and <code class="language-plaintext highlighter-rouge">predictt</code> to sample from the conditional distribution and to get point estimate ($\mu_{y_* \mid D}$) of the conditional distribution respectively.</p>

<p>Let’s see each method in detail in the following sections:</p>

<h3 id="marginal_likelihood-method"><code class="language-plaintext highlighter-rouge">marginal_likelihood</code> method</h3>

<p>First, we need to instantiate the model using a kernel function and (optionally) a mean function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pymc4.gp</span> <span class="kn">import</span> <span class="n">MarginalGP</span>
<span class="kn">from</span> <span class="nn">pymc4.gp.cov</span> <span class="kn">import</span> <span class="n">ExpQuad</span>

<span class="c1"># Let's first instantiate a kernel
</span><span class="n">K</span> <span class="o">=</span> <span class="n">ExpQuad</span><span class="p">(</span><span class="n">length_scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span>

<span class="c1"># Now, we can instantiata the model
</span><span class="n">gp</span> <span class="o">=</span> <span class="n">MarginalGP</span><span class="p">(</span><span class="n">cov_fn</span> <span class="o">=</span> <span class="n">K</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, To get the <code class="language-plaintext highlighter-rouge">marginal_likelihood</code> of the MarginalGP over some data <code class="language-plaintext highlighter-rouge">X</code> (of shape <code class="language-plaintext highlighter-rouge">(n_samples, n_features)</code>) with labels <code class="language-plaintext highlighter-rouge">y</code> (of shape <code class="language-plaintext highlighter-rouge">(n_samples, )</code>), use:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">noise</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s">"y_"</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
</code></pre></div></div>

<p>You can also pass a covariance object as noise to the <code class="language-plaintext highlighter-rouge">marginal_likelihood</code>
method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># kronecker delta function with epsilon 1e-2
</span><span class="n">noise</span> <span class="o">=</span> <span class="n">WhiteNoise</span><span class="p">(</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s">"y_"</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
</code></pre></div></div>

<p>If <code class="language-plaintext highlighter-rouge">y</code> is not the observed data, pass <code class="language-plaintext highlighter-rouge">is_observed=False</code> in the
        marginal likelihood method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s">"y_"</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span> <span class="n">is_observed</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p>By default, some <code class="language-plaintext highlighter-rouge">jitter</code> is added to ensure Cholesky Decomposition passes.
        This behavior can be turned off by passing <code class="language-plaintext highlighter-rouge">jitter=0</code> in the marginal
        likelihood method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s">"y_"</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>As <code class="language-plaintext highlighter-rouge">noise</code> behaves exactly as <code class="language-plaintext highlighter-rouge">jitter</code>, it is recommended to set <code class="language-plaintext highlighter-rouge">jitter=False</code>
        to avoid adding extra noise.</p>

<h3 id="conditional-method"><code class="language-plaintext highlighter-rouge">conditional</code> method</h3>

<p>You can use conditional method to get the conditional distribution
        over the new data points. This distribution can be used to predict
        over the new data points:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">conditional</span><span class="p">(</span><span class="s">"y_pred"</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="n">given</span><span class="o">=</span><span class="p">{</span><span class="s">"X"</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="s">"y"</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="s">"noise"</span><span class="p">:</span> <span class="n">noise</span><span class="p">})</span>
</code></pre></div></div>

<p>where <code class="language-plaintext highlighter-rouge">Xnew</code> are the test points (new data points) and <code class="language-plaintext highlighter-rouge">y_pred</code> is a multivariate normal distribution. <code class="language-plaintext highlighter-rouge">given</code> dictionary is optional when <code class="language-plaintext highlighter-rouge">marginal_likelihood</code> method has been called before.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s">"y_"</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">conditional</span><span class="p">(</span><span class="s">"y_pred"</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">)</span> <span class="c1"># no need to pass given
</span></code></pre></div></div>

<p>To add noise in the conditional distribution, use:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">conditional</span><span class="p">(</span><span class="s">"y_pred"</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="n">pred_noise</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>To avoid reparametrizing to the <code class="language-plaintext highlighter-rouge">MvNormalCholesky</code> distribution, use:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">conditional</span><span class="p">(</span><span class="s">"y_pred"</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">,</span> <span class="n">reparametrize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="notes">Notes</h3>

<p>The data and the parameters <strong>must have the same datatype</strong>. For example, if the data is represented as <code class="language-plaintext highlighter-rouge">float64</code> then all the parameters must also be represented as <code class="language-plaintext highlighter-rouge">float64</code> datatype only. Using different datatypes in-between the model will raise an exception.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc4</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">from</span> <span class="nn">pymc4.gp</span> <span class="kn">import</span> <span class="n">MarginalGP</span>
<span class="kn">from</span> <span class="nn">pymc4.gp.cov</span> <span class="kn">import</span> <span class="n">ExpQuad</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">KernelPCA</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s">'retina'</span>
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">8927</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">(</span><span class="s">'arviz-darkgrid'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="example-1--gp-lvm-using-a-linear-kernel">Example 1 : GP-LVM using a Linear Kernel</h3>

<p>Marginal Gaussian Processes can be used to perform Principal Component Analysis (PCA) (a technique to project and visualize high dimensional data onto a low dimensional feature space) using a model called Gaussian Process Latent Variable Model (GP-LVM). GP-LVMs generally outperform the native PCS algorithm and can even generalize further using non-linear kernel functions like RBF kernel. This property of GP-LVMs makes them more auspicious over vanilla PCA. To learn more about GP-LVMs, refer [1].</p>

<p>Below is an example of a Linear GP-LVM that projects the Iris Flower Dataset onto two-dimensional feature space while preserving a clear separation between different types of flowers.</p>

<p><strong>References</strong></p>

<p>[1] Neil D. Lawrence, 2003, Gaussian process latent variable models for visualization of high dimensional data, https://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load the data. Needs sklearn - Use `pip install sklearn` inside the python
# environment/conda prompt/terminal to install sklearn locally. 
</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

<span class="n">N</span><span class="p">,</span> <span class="n">P</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Linear GP-LVM model
</span><span class="o">@</span><span class="n">pm</span><span class="p">.</span><span class="n">model</span>
<span class="k">def</span> <span class="nf">GPLVM</span><span class="p">():</span>
    <span class="c1"># A normal prior over the projected data
</span>    <span class="n">x</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvNormalCholesky</span><span class="p">(</span><span class="s">'x'</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">batch_stack</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Prior over the argumemnts of the covariance function.
</span>    <span class="n">args</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'args'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="mf">5.</span><span class="p">),</span> <span class="n">batch_stack</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">cov_fn</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="n">cov</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="n">MarginalGP</span><span class="p">(</span><span class="n">cov_fn</span><span class="o">=</span><span class="n">cov_fn</span><span class="p">)</span>

    <span class="c1"># We put a marginal likelihood over every feature in the dataset.
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">gp</span><span class="p">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s">f'y</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">noise</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.01</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPLVM</span><span class="p">()</span>
<span class="n">advi</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="mi">15000</span>
<span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">advi</span><span class="p">.</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of iterations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Negative ELBO'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">trace</span> <span class="o">=</span> <span class="n">advi</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">sample</span><span class="p">(</span><span class="mi">15000</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'GPLVM/x'</span><span class="p">])</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">xx</span><span class="p">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xx</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/GP-Marginal_files/GP-Marginal_6_1.png" alt="png" /></p>

<p><img src="/images/GP-Marginal_files/GP-Marginal_6_2.png" alt="png" /></p>

<h3 id="gp-lvms-with-a-non-linear-kernel">GP-LVMs with a non-linear kernel</h3>

<p>Below is the demonstration of a GP-LVM using a non-linear RBF kernel and a comparison with the sklearn’s <code class="language-plaintext highlighter-rouge">KernelPCA</code> model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Experiments with Non-Linear Kernels
</span><span class="o">@</span><span class="n">pm</span><span class="p">.</span><span class="n">model</span>
<span class="k">def</span> <span class="nf">GPLVM</span><span class="p">():</span>
    <span class="c1"># A normal prior over the projected data
</span>    <span class="n">x</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvNormalCholesky</span><span class="p">(</span><span class="s">'x'</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">batch_stack</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Prior over the argumemnts of the covariance function.
</span>    <span class="n">args</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s">'args'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="mf">100.0</span><span class="p">),</span> <span class="n">batch_stack</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">cov_fn</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="n">cov</span><span class="p">.</span><span class="n">ExpQuad</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="n">MarginalGP</span><span class="p">(</span><span class="n">cov_fn</span><span class="o">=</span><span class="n">cov_fn</span><span class="p">)</span>

    <span class="c1"># We put a marginal likelihood over every feature in the dataset.
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">P</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">gp</span><span class="p">.</span><span class="n">marginal_likelihood</span><span class="p">(</span><span class="s">f'y_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">noise</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPLVM</span><span class="p">()</span>
<span class="n">advi</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">15000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">advi</span><span class="p">.</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of iterations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Negative ELBO'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">trace</span> <span class="o">=</span> <span class="n">advi</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">sample</span><span class="p">(</span><span class="mi">15000</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'GPLVM/x'</span><span class="p">])</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">xx</span><span class="p">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xx</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/GP-Marginal_files/GP-Marginal_8_0.png" alt="png" /></p>

<p><img src="/images/GP-Marginal_files/GP-Marginal_8_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Comparing with sklearn
</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">KernelPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s">'rbf'</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xx</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/GP-Marginal_files/GP-Marginal_9_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
<span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span>
</code></pre></div></div>

<pre><code class="language-none">pymc4      4.0a2
arviz      0.9.0
tensorflow 2.4.0-dev20200705
numpy      1.19.0
last updated: Fri Aug 07 2020 

CPython 3.8.0
IPython 7.16.1
watermark 2.0.2
</code></pre>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/tags/#gaussian-processes" class="page__taxonomy-item" rel="tag">Gaussian Processes</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#gsoc-2020" class="page__taxonomy-item" rel="tag">GSoC 2020</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#machine-learning" class="page__taxonomy-item" rel="tag">Machine Learning</a>
    
    </span>
  </p>




  






  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/categories/#gaussian-processes" class="page__taxonomy-item" rel="tag">Gaussian Processes</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/categories/#gsoc-2020" class="page__taxonomy-item" rel="tag">GSoC 2020</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/categories/#machine-learning" class="page__taxonomy-item" rel="tag">Machine Learning</a>
    
    </span>
  </p>


      </footer>

      

      


  <nav class="pagination">
    
      <a href="http://localhost:4000/gsoc2020/gsoc-phase-2-summary" class="pagination--pager" title="GSoC’20 Phase 2 Report
">Previous</a>
    
    
      <a href="http://localhost:4000/gsoc2020/gsoc-final-report" class="pagination--pager" title="GSoC’20 Final Report
">Next</a>
    
  </nav>

    </div>

    
      

<div class="page__comments">
  
  
</div>
    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/tirthasheshpatel"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Tirth Patel. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-161186729-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>










  </body>
</html>

